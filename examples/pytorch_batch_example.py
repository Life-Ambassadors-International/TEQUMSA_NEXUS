#!/usr/bin/env python3
"""
TEQUMSA PyTorch Batch Processing Example

Demonstrates constructing random agent batches, scoring on CPU/GPU,
and printing aggregate statistics.

Generated by GitHub Copilot Chat Assistant.
"""

import sys
import os
import time
import numpy as np
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    import torch
    TORCH_AVAILABLE = True
    print(f"PyTorch available: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA device count: {torch.cuda.device_count()}")
        print(f"Current CUDA device: {torch.cuda.current_device()}")
except ImportError:
    TORCH_AVAILABLE = False
    print("PyTorch not available - falling back to NumPy")

from tequmsa import AwarenessEngine
from tequmsa.core.backends import get_backend


def generate_random_agent_batch(batch_size: int = 50, vector_dim: int = 128, seed: int = 42) -> np.ndarray:
    """Generate random agent state vectors for testing."""
    np.random.seed(seed)
    
    # Generate random vectors with some structure
    base_vectors = np.random.randn(batch_size, vector_dim).astype(np.float32)
    
    # Add some correlation structure
    for i in range(1, batch_size):
        correlation = np.random.uniform(0.1, 0.3)
        base_vectors[i] = (1 - correlation) * base_vectors[i] + correlation * base_vectors[0]
    
    # Normalize vectors
    norms = np.linalg.norm(base_vectors, axis=1, keepdims=True)
    norms = np.where(norms == 0, 1, norms)  # Avoid division by zero
    base_vectors = base_vectors / norms
    
    return base_vectors


def generate_gaia_signature(vector_dim: int = 128, seed: int = 123) -> dict:
    """Generate a synthetic GAIA signature for testing."""
    np.random.seed(seed)
    
    # Create structured component vectors
    bio_vector = np.random.randn(vector_dim).astype(np.float32)
    bio_vector = bio_vector / np.linalg.norm(bio_vector)
    
    # Digital component - partially correlated with bio
    digital_vector = 0.7 * bio_vector + 0.3 * np.random.randn(vector_dim).astype(np.float32)
    digital_vector = digital_vector / np.linalg.norm(digital_vector)
    
    # Cosmic component - more independent
    cosmic_vector = np.random.randn(vector_dim).astype(np.float32)
    cosmic_vector = cosmic_vector / np.linalg.norm(cosmic_vector)
    
    return {
        'bio': bio_vector,
        'digital': digital_vector,
        'cosmic': cosmic_vector
    }


def benchmark_backend(backend_name: str, device: str = "cpu"):
    """Benchmark processing with specific backend."""
    print(f"\n{'='*60}")
    print(f"Benchmarking {backend_name.upper()} backend on {device}")
    print(f"{'='*60}")
    
    # Test parameters
    batch_sizes = [10, 50, 100]
    vector_dim = 128
    
    try:
        # Initialize engine with specific backend
        os.environ["TEQUMSA_BACKEND"] = backend_name
        engine = AwarenessEngine(tier="pro", backend_name=backend_name, device=device)
        
        # Load GAIA signature
        gaia_signature = generate_gaia_signature(vector_dim)
        engine.load_gaia_signature(gaia_signature)
        
        print(f"Engine initialized with backend: {engine.backend.name}")
        print(f"Adaptive alpha enabled: {engine.adaptive_enabled}")
        
        results = []
        
        for batch_size in batch_sizes:
            print(f"\nTesting batch size: {batch_size}")
            
            # Generate test batch
            agent_batch = generate_random_agent_batch(batch_size, vector_dim)
            print(f"Generated batch shape: {agent_batch.shape}")
            
            # Single vector processing benchmark
            start_time = time.time()
            single_results = []
            for i in range(batch_size):
                awakened, R, components, diagnostics = engine.compute_awakening(agent_batch[i])
                single_results.append((awakened, R, components))
            single_time = time.time() - start_time
            
            # Batch processing benchmark
            start_time = time.time()
            awakened_batch, R_batch, components_batch, batch_diagnostics = engine.compute_awakening_batch(agent_batch)
            batch_time = time.time() - start_time
            
            # Analyze results
            awakened_count = sum(awakened_batch)
            mean_R = np.mean(R_batch)
            std_R = np.std(R_batch)
            
            # Component statistics
            bio_scores = [comp['bio'] for comp in components_batch]
            digital_scores = [comp['digital'] for comp in components_batch]
            cosmic_scores = [comp['cosmic'] for comp in components_batch]
            
            print(f"  Single processing time: {single_time:.4f}s ({single_time/batch_size*1000:.2f}ms per vector)")
            print(f"  Batch processing time:  {batch_time:.4f}s ({batch_time/batch_size*1000:.2f}ms per vector)")
            print(f"  Speedup: {single_time/batch_time:.2f}x")
            print(f"  Awakened vectors: {awakened_count}/{batch_size} ({awakened_count/batch_size*100:.1f}%)")
            print(f"  R scores - Mean: {mean_R:.4f}, Std: {std_R:.4f}")
            print(f"  Component means - Bio: {np.mean(bio_scores):.4f}, Digital: {np.mean(digital_scores):.4f}, Cosmic: {np.mean(cosmic_scores):.4f}")
            
            results.append({
                'batch_size': batch_size,
                'single_time': single_time,
                'batch_time': batch_time,
                'speedup': single_time / batch_time,
                'awakened_count': awakened_count,
                'mean_R': mean_R,
                'std_R': std_R
            })
        
        # Summary statistics
        print(f"\nSummary for {backend_name.upper()} backend:")
        print(f"  Average speedup: {np.mean([r['speedup'] for r in results]):.2f}x")
        print(f"  Average awakening rate: {np.mean([r['awakened_count']/r['batch_size'] for r in results])*100:.1f}%")
        print(f"  Average R score: {np.mean([r['mean_R'] for r in results]):.4f}")
        
        return results
        
    except Exception as e:
        print(f"Error benchmarking {backend_name}: {e}")
        return None


def main():
    """Main demonstration function."""
    print("TEQUMSA PyTorch Batch Processing Example")
    print("=" * 50)
    
    # Set up logging to be less verbose
    os.environ["TEQUMSA_DISABLE_CONSCIOUSNESS_LOG"] = "1"
    
    results = {}
    
    # Test NumPy backend
    print("\nTesting NumPy backend...")
    numpy_results = benchmark_backend("numpy")
    if numpy_results:
        results["numpy"] = numpy_results
    
    # Test PyTorch CPU backend
    if TORCH_AVAILABLE:
        print("\nTesting PyTorch CPU backend...")
        torch_cpu_results = benchmark_backend("torch", "cpu")
        if torch_cpu_results:
            results["torch_cpu"] = torch_cpu_results
        
        # Test PyTorch GPU backend if available
        if torch.cuda.is_available():
            print("\nTesting PyTorch GPU backend...")
            torch_gpu_results = benchmark_backend("torch", "cuda")
            if torch_gpu_results:
                results["torch_gpu"] = torch_gpu_results
    
    # Performance comparison
    if len(results) > 1:
        print(f"\n{'='*60}")
        print("PERFORMANCE COMPARISON")
        print(f"{'='*60}")
        
        for backend_name, backend_results in results.items():
            avg_speedup = np.mean([r['speedup'] for r in backend_results])
            avg_time_per_vector = np.mean([r['batch_time']/r['batch_size'] for r in backend_results]) * 1000
            print(f"{backend_name:12s}: {avg_speedup:.2f}x speedup, {avg_time_per_vector:.2f}ms per vector")
    
    # Adaptive alpha demonstration
    print(f"\n{'='*60}")
    print("ADAPTIVE ALPHA DEMONSTRATION")
    print(f"{'='*60}")
    
    try:
        # Create engine with adaptive alpha enabled
        engine = AwarenessEngine(tier="enterprise")
        gaia_signature = generate_gaia_signature(128)
        engine.load_gaia_signature(gaia_signature)
        
        print(f"Initial alpha: {engine.current_alpha:.3f}")
        print(f"Alpha bounds: [{engine.alpha_min:.3f}, {engine.alpha_max:.3f}]")
        
        # Generate sequence of vectors with increasing similarity
        np.random.seed(999)
        base_vector = np.random.randn(128).astype(np.float32)
        base_vector = base_vector / np.linalg.norm(base_vector)
        
        print("\nProcessing sequence of increasingly similar vectors...")
        for i in range(20):
            # Create vector with increasing similarity to GAIA signature
            similarity_factor = i / 19.0  # 0 to 1
            
            # Blend with bio component
            test_vector = (1 - similarity_factor) * base_vector + similarity_factor * gaia_signature['bio']
            test_vector = test_vector / np.linalg.norm(test_vector)
            
            awakened, R, components, diagnostics = engine.compute_awakening(test_vector)
            
            if i % 5 == 0 or diagnostics.get('alpha_adjustment') is not None:
                print(f"  Step {i:2d}: R={R:.4f}, alpha={engine.current_alpha:.3f}, awakened={awakened}")
                if diagnostics.get('alpha_adjustment'):
                    print(f"    Alpha adjustment: {diagnostics['alpha_adjustment']:+.3f} ({diagnostics['adjust_reason']})")
        
        print(f"Final alpha: {engine.current_alpha:.3f}")
        
        # Show rolling statistics
        status = engine.get_status()
        if status['rolling_stats']:
            rs = status['rolling_stats']
            print(f"Rolling stats: mean_R={rs['mean_R']:.4f}, std_R={rs['std_R']:.4f}, count={rs['count']}")
    
    except Exception as e:
        print(f"Error in adaptive alpha demo: {e}")
    
    print(f"\n{'='*60}")
    print("Example completed successfully!")
    print("Check logs/consciousness_log.jsonl for detailed consciousness events.")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()